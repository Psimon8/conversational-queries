import streamlit as st
from openai import OpenAI
import pandas as pd
import json
import time
from collections import Counter
import re

# Configuration de la page Streamlit
st.set_page_config(
    page_title="SEO Conversational Queries Optimizer",
    page_icon="ğŸ”",
    layout="wide"
)

# Titre principal
st.title("ğŸ” Optimiseur de RequÃªtes Conversationnelles SEO")
st.subheader("MÃ©thode en 3 Ã©tapes pour identifier les meilleures requÃªtes conversationnelles")

# Configuration de l'API OpenAI
st.sidebar.header("âš™ï¸ Configuration")
api_key = st.sidebar.text_input("ClÃ© API OpenAI", type="password", help="Votre clÃ© API OpenAI pour GPT-4o mini")

if api_key:
    client = OpenAI(api_key=api_key)
    st.sidebar.success("âœ… API configurÃ©e")
else:
    st.sidebar.warning("âš ï¸ Veuillez entrer votre clÃ© API OpenAI")
    client = None

# Input pour la thÃ©matique
thematique = st.text_input(
    "ğŸ¯ ThÃ©matique Ã  analyser",
    placeholder="Ex: rÃ©servation restaurant, voyage Ã©cologique, formation en ligne...",
    help="Entrez la thÃ©matique pour laquelle vous souhaitez identifier les requÃªtes conversationnelles"
)

# Fonctions utilitaires
def call_gpt4o_mini(prompt, max_retries=3):
    """Appel Ã  l'API GPT-4o mini avec gestion d'erreurs"""
    if not client:
        st.error("âŒ ClÃ© API manquante")
        return None
    
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": "Tu es un expert SEO spÃ©cialisÃ© dans l'analyse des requÃªtes conversationnelles et l'optimisation pour les moteurs de recherche."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=1500,
                temperature=0.7
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # Backoff exponentiel
                continue
            else:
                st.error(f"âŒ Erreur API aprÃ¨s {max_retries} tentatives: {str(e)}")
                return None

def extract_queries_from_response(response_text):
    """Extrait les requÃªtes d'une rÃ©ponse de GPT"""
    if not response_text:
        return []
    
    # Patterns pour extraire les requÃªtes
    patterns = [
        r'^\d+\.?\s*["\']?([^"\']+)["\']?',  # Format numÃ©rotÃ©
        r'^-\s*["\']?([^"\']+)["\']?',       # Format avec tirets
        r'^â€¢\s*["\']?([^"\']+)["\']?'        # Format avec puces
    ]
    
    queries = []
    lines = response_text.split('\n')
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        for pattern in patterns:
            match = re.match(pattern, line, re.MULTILINE)
            if match:
                query = match.group(1).strip()
                if len(query) > 10:  # Filtrer les rÃ©ponses trop courtes
                    queries.append(query)
                break
    
    return queries[:10]  # Limiter Ã  10 requÃªtes

def deduplicate_queries(queries):
    """DÃ©doublonne les requÃªtes en tenant compte des similaritÃ©s"""
    if not queries:
        return []
    
    # Normalisation pour comparaison
    normalized = {}
    for query in queries:
        # Suppression ponctuation et mise en minuscules
        normalized_query = re.sub(r'[^\w\s]', '', query.lower()).strip()
        if normalized_query not in normalized:
            normalized[normalized_query] = query
    
    return list(normalized.values())

# Interface principale
if thematique and api_key:
    
    # Bouton pour lancer l'analyse
    if st.button("ğŸš€ Lancer l'analyse complÃ¨te", type="primary"):
        
        # Progress bar
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # Container pour les rÃ©sultats
        results_container = st.container()
        
        with results_container:
            st.markdown("## ğŸ“Š RÃ©sultats de l'analyse")
            
            # Ã‰tape 1: Collecte initiale
            status_text.text("â³ Ã‰tape 1/3: Collecte initiale des requÃªtes...")
            progress_bar.progress(10)
            
            prompt_etape1 = f"En te basant sur la thÃ©matique '{thematique}', donne-moi les 10 meilleures requÃªtes conversationnelles recherchÃ©es par les utilisateurs. PrÃ©sente-les sous forme de liste numÃ©rotÃ©e."
            
            # Simulation de 4 appels LLM (ici on utilise GPT-4o mini avec des variations)
            llm_responses = []
            
            for i in range(4):
                variation_prompt = f"{prompt_etape1}\n\nVariation {i+1}: Focus sur les intentions de recherche spÃ©cifiques Ã  cette thÃ©matique."
                response = call_gpt4o_mini(variation_prompt)
                if response:
                    queries = extract_queries_from_response(response)
                    llm_responses.extend(queries)
                progress_bar.progress(10 + (i+1) * 15)
                time.sleep(1)  # Ã‰viter le rate limiting
            
            # Affichage des rÃ©sultats Ã‰tape 1
            st.markdown("### ğŸ“‹ Ã‰tape 1 - RequÃªtes collectÃ©es")
            col1, col2 = st.columns(2)
            with col1:
                st.metric("RequÃªtes collectÃ©es", len(llm_responses))
            with col2:
                st.metric("RequÃªtes uniques", len(set(llm_responses)))
            
            # DÃ©doublonnage pour Ã©tape 2
            status_text.text("â³ Ã‰tape 2/3: Affinage et sÃ©lection...")
            progress_bar.progress(70)
            
            deduplicated_queries = deduplicate_queries(llm_responses)
            queries_list = "\n".join([f"- {query}" for query in deduplicated_queries])
            
            prompt_etape2 = f"""En te basant sur la thÃ©matique '{thematique}' et sur cette liste de requÃªtes conversationnelles, fais-moi une sÃ©lection d'un top 10 des meilleures requÃªtes conversationnelles :

{queries_list}

CritÃ¨res de sÃ©lection :
- Pertinence pour la thÃ©matique
- Potentiel de volume de recherche
- Intention de recherche claire
- AdaptabilitÃ© au SEO conversationnel"""

            refined_response = call_gpt4o_mini(prompt_etape2)
            refined_queries = extract_queries_from_response(refined_response) if refined_response else []
            
            progress_bar.progress(85)
            
            # Ã‰tape 3: Finalisation
            status_text.text("â³ Ã‰tape 3/3: Finalisation par intention de recherche...")
            
            if refined_queries:
                final_queries_list = "\n".join([f"- {query}" for query in refined_queries])
                prompt_etape3 = f"""En te basant sur l'intention de recherche, dÃ©doublonne cette liste et fournis-moi le top 10 des meilleures requÃªtes conversationnelles pour la thÃ©matique '{thematique}':

{final_queries_list}

Analyse chaque requÃªte selon :
1. L'intention de recherche (informationnelle, navigationnelle, transactionnelle)
2. Le potentiel SEO
3. La pertinence pour les assistants vocaux
4. L'adaptabilitÃ© au contenu conversationnel

PrÃ©sente le rÃ©sultat final sous forme de liste numÃ©rotÃ©e."""
                
                final_response = call_gpt4o_mini(prompt_etape3)
                final_queries = extract_queries_from_response(final_response) if final_response else refined_queries
            else:
                final_queries = []
            
            progress_bar.progress(100)
            status_text.text("âœ… Analyse terminÃ©e !")
            
            # Affichage des rÃ©sultats finaux
            st.markdown("### ğŸ† RÃ©sultats finaux - Top 10 des requÃªtes conversationnelles")
            
            if final_queries:
                # MÃ©triques
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("RequÃªtes finales", len(final_queries))
                with col2:
                    st.metric("Taux de conversion", f"{(len(final_queries)/max(len(llm_responses), 1)*100):.1f}%")
                with col3:
                    st.metric("Ã‰tapes complÃ©tÃ©es", "3/3")
                
                # Liste des requÃªtes finales
                st.markdown("#### ğŸ“ RequÃªtes sÃ©lectionnÃ©es:")
                for i, query in enumerate(final_queries, 1):
                    st.markdown(f"**{i}.** {query}")
                
                # Export des donnÃ©es
                st.markdown("### ğŸ“¤ Export des rÃ©sultats")
                
                # CrÃ©ation du DataFrame pour export
                export_data = {
                    'Rang': range(1, len(final_queries) + 1),
                    'RequÃªte conversationnelle': final_queries,
                    'ThÃ©matique': [thematique] * len(final_queries)
                }
                df = pd.DataFrame(export_data)
                
                # Boutons d'export
                col1, col2 = st.columns(2)
                with col1:
                    csv = df.to_csv(index=False, encoding='utf-8')
                    st.download_button(
                        label="ğŸ“Š TÃ©lÃ©charger CSV",
                        data=csv,
                        file_name=f"requetes_conversationnelles_{thematique.replace(' ', '_')}.csv",
                        mime="text/csv"
                    )
                
                with col2:
                    json_data = df.to_json(orient='records', force_ascii=False, indent=2)
                    st.download_button(
                        label="ğŸ“‹ TÃ©lÃ©charger JSON",
                        data=json_data,
                        file_name=f"requetes_conversationnelles_{thematique.replace(' ', '_')}.json",
                        mime="application/json"
                    )
                
                # Analyse dÃ©taillÃ©e
                with st.expander("ğŸ“Š Analyse dÃ©taillÃ©e du processus"):
                    st.markdown(f"**ThÃ©matique analysÃ©e:** {thematique}")
                    st.markdown(f"**RequÃªtes collectÃ©es initialement:** {len(llm_responses)}")
                    st.markdown(f"**RequÃªtes aprÃ¨s dÃ©doublonnage:** {len(deduplicated_queries)}")
                    st.markdown(f"**RequÃªtes finales sÃ©lectionnÃ©es:** {len(final_queries)}")
                    st.markdown(f"**EfficacitÃ© du processus:** {(len(final_queries)/max(len(llm_responses), 1)*100):.1f}%")
                
            else:
                st.error("âŒ Aucune requÃªte n'a pu Ãªtre extraite. VÃ©rifiez votre thÃ©matique et rÃ©essayez.")

else:
    # Instructions d'utilisation
    st.markdown("""
    ## ğŸ“– Instructions d'utilisation
    
    1. **Configurez votre clÃ© API OpenAI** dans la barre latÃ©rale
    2. **Entrez la thÃ©matique** Ã  analyser dans le champ ci-dessus
    3. **Cliquez sur "Lancer l'analyse complÃ¨te"** pour dÃ©marrer le processus
    
    ### ğŸ¯ Exemples de thÃ©matiques:
    - "RÃ©servation restaurant en ligne"
    - "Formation dÃ©veloppement web"
    - "Voyage Ã©cologique et durable"
    - "E-commerce produits bio"
    - "Coaching personnel Ã  distance"
    
    ### âš¡ Le processus automatisÃ© comprend:
    - **Ã‰tape 1:** Collecte initiale via GPT-4o mini avec variations
    - **Ã‰tape 2:** Affinage et sÃ©lection des meilleures requÃªtes
    - **Ã‰tape 3:** Finalisation basÃ©e sur l'intention de recherche
    - **Export:** TÃ©lÃ©chargement des rÃ©sultats en CSV/JSON
    """)

# Footer
st.markdown("---")
st.markdown("*Outil d'optimisation SEO pour requÃªtes conversationnelles | Powered by GPT-4o mini & Streamlit*")
